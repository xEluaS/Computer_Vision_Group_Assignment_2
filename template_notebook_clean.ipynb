{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7400e183",
   "metadata": {
    "_cell_guid": "b47b15de-64a5-4fa9-a688-23d3efa9a2f4",
    "_uuid": "0cc385a7-98f6-4883-96eb-7b89c7c9aa1c",
    "papermill": {
     "duration": 0.016533,
     "end_time": "2022-04-12T14:48:23.471825",
     "exception": false,
     "start_time": "2022-04-12T14:48:23.455292",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"width:100%; height:140px\">\n",
    "    <img src=\"https://www.kuleuven.be/communicatie/marketing/aanbesteding/artwork-textiel/kuleuven-logo.png\" width = \"300px\" heigh = \"auto\" align=\"left\">\n",
    "</div>\n",
    "\n",
    "\n",
    "KUL H02A5a Computer Vision: Group Assignment 2\n",
    "---------------------------------------------------------------\n",
    "Student numbers: r0902260, r0977251, r1003576, r1005663, r0375658.\n",
    "\n",
    "In this group assignment your team will delve into some deep learning applications for computer vision. The assignment will be delivered in the same groups from *Group assignment 1* and you start from this template notebook. The notebook you submit for grading is the last notebook you submit in the [Kaggle competition](https://www.kaggle.com/t/d11be6a431b84198bc85f54ae7e2563f) prior to the deadline on **Tuesday 24 May 23:59**. Closely follow [these instructions](https://github.com/gourie/kaggle_inclass) for joining the competition, sharing your notebook with the TAs and making a valid notebook submission to the competition. A notebook submission not only produces a *submission.csv* file that is used to calculate your competition score, it also runs the entire notebook and saves its output as if it were a report. This way it becomes an all-in-one-place document for the TAs to review. As such, please make sure that your final submission notebook is self-contained and fully documented (e.g. provide strong arguments for the design choices that you make). Most likely, this notebook format is not appropriate to run all your experiments at submission time (e.g. the training of CNNs is a memory hungry and time consuming process; due to limited Kaggle resources). It can be a good idea to distribute your code otherwise and only summarize your findings, together with your final predictions, in the submission notebook. For example, you can substitute experiments with some text and figures that you have produced \"offline\" (e.g. learning curves and results on your internal validation set or even the test set for different architectures, pre-processing pipelines, etc). We advise you to first go through the PDF of this assignment entirely before you really start. Then, it can be a good idea to go through this notebook and use it as your first notebook submission to the competition. You can make use of the *Group assignment 2* forum/discussion board on Toledo if you have any questions. Good luck and have fun!\n",
    "\n",
    "---------------------------------------------------------------\n",
    "NOTES:\n",
    "* This notebook is just a template. Please keep the five main sections, but feel free to adjust further in any way you please!\n",
    "* Clearly indicate the improvements that you make! You can for instance use subsections like: *3.1. Improvement: applying loss function f instead of g*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27e4eb4",
   "metadata": {
    "_cell_guid": "35358cfb-b13d-4277-8dd5-4e663c8cd775",
    "_uuid": "3b40b846-d7da-46d8-b354-c6d5c5ded56e",
    "papermill": {
     "duration": 0.014397,
     "end_time": "2022-04-12T14:48:23.501501",
     "exception": false,
     "start_time": "2022-04-12T14:48:23.487104",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Overview\n",
    "This assignment consists of *three main parts* for which we expect you to provide code and extensive documentation in the notebook:\n",
    "* Image classification (Sect. 2)\n",
    "* Semantic segmentation (Sect. 3)\n",
    "* Adversarial attacks (Sect. 4)\n",
    "\n",
    "In the first part, you will train an end-to-end neural network for image classification. In the second part, you will do the same for semantic segmentation. For these two tasks we expect you to put a significant effort into optimizing performance and as such competing with fellow students via the Kaggle competition. In the third part, you will try to find and exploit the weaknesses of your classification and/or segmentation network. For the latter there is no competition format, but we do expect you to put significant effort in achieving good performance on the self-posed goal for that part. Finally, we ask you to reflect and produce an overall discussion with links to the lectures and \"real world\" computer vision (Sect. 5). It is important to note that only a small part of the grade will reflect the actual performance of your networks. However, we do expect all things to work! In general, we will evaluate the correctness of your approach and your understanding of what you have done that you demonstrate in the descriptions and discussions in the final notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96360d34",
   "metadata": {
    "papermill": {
     "duration": 0.014263,
     "end_time": "2022-04-12T14:48:23.530341",
     "exception": false,
     "start_time": "2022-04-12T14:48:23.516078",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.1 Deep learning resources\n",
    "If you did not yet explore this in *Group assignment 1 (Sect. 2)*, we recommend using the TensorFlow and/or Keras library for building deep learning models. You can find a nice crash course [here](https://colab.research.google.com/drive/1UCJt8EYjlzCs1H1d1X0iDGYJsHKwu-NO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79973719",
   "metadata": {
    "_cell_guid": "7ddf657a-b938-4a49-87dc-b0db9af9156d",
    "_uuid": "c65ea4f1-cc90-408f-b8e0-7c7399ec7e21",
    "execution": {
     "iopub.execute_input": "2022-04-12T14:48:23.569550Z",
     "iopub.status.busy": "2022-04-12T14:48:23.568937Z",
     "iopub.status.idle": "2022-04-12T14:48:28.961335Z",
     "shell.execute_reply": "2022-04-12T14:48:28.960550Z",
     "shell.execute_reply.started": "2022-04-12T14:40:37.124801Z"
    },
    "papermill": {
     "duration": 5.416492,
     "end_time": "2022-04-12T14:48:28.961510",
     "exception": false,
     "start_time": "2022-04-12T14:48:23.545018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f890505",
   "metadata": {
    "papermill": {
     "duration": 0.014416,
     "end_time": "2022-04-12T14:48:28.990998",
     "exception": false,
     "start_time": "2022-04-12T14:48:28.976582",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.2 PASCAL VOC 2009\n",
    "For this project you will be using the [PASCAL VOC 2009](http://host.robots.ox.ac.uk/pascal/VOC/voc2009/index.html) dataset. This dataset consists of colour images of various scenes with different object classes (e.g. animal: *bird, cat, ...*; vehicle: *aeroplane, bicycle, ...*), totalling 20 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5b387a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the training data\n",
    "train_df = pd.read_csv('/kaggle/input/kul-h02a5a-computer-vision-ga2-2022/train/train_set.csv', index_col=\"Id\")\n",
    "labels = train_df.columns\n",
    "train_df[\"img\"] = [np.load('/kaggle/input/kul-h02a5a-computer-vision-ga2-2022/train/img/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\n",
    "train_df[\"seg\"] = [np.load('/kaggle/input/kul-h02a5a-computer-vision-ga2-2022/train/seg/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\n",
    "print(\"The training set contains {} examples.\".format(len(train_df)))\n",
    "\n",
    "# Show some examples\n",
    "fig, axs = plt.subplots(2, 20, figsize=(10 * 20, 10 * 2))\n",
    "for i, label in enumerate(labels):\n",
    "    df = train_df.loc[train_df[label] == 1]\n",
    "    axs[0, i].imshow(df.iloc[0][\"img\"], vmin=0, vmax=255)\n",
    "    axs[0, i].set_title(\"\\n\".join(label for label in labels if df.iloc[0][label] == 1), fontsize=40)\n",
    "    axs[0, i].axis(\"off\")\n",
    "    axs[1, i].imshow(df.iloc[0][\"seg\"], vmin=0, vmax=20)  # with the absolute color scale it will be clear that the arrays in the \"seg\" column are label maps (labels in [0, 20])\n",
    "    axs[1, i].axis(\"off\")\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "# The training dataframe contains for each image 20 columns with the ground truth classification labels and 20 column with the ground truth segmentation maps for each class\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db2741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take first 20 columns the values of 0 and 1 of the dataframe as labels\n",
    "labels_df = train_df.iloc[:, :20]\n",
    "labels_train = labels_df.values\n",
    "print(labels_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813f5c92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-12T14:48:50.732979Z",
     "iopub.status.busy": "2022-04-12T14:48:50.731962Z",
     "iopub.status.idle": "2022-04-12T14:49:02.044076Z",
     "shell.execute_reply": "2022-04-12T14:49:02.043459Z",
     "shell.execute_reply.started": "2022-04-12T14:43:34.934176Z"
    },
    "papermill": {
     "duration": 11.507733,
     "end_time": "2022-04-12T14:49:02.044233",
     "exception": false,
     "start_time": "2022-04-12T14:48:50.536500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading the test data\n",
    "test_df = pd.read_csv('/kaggle/input/kul-h02a5a-computer-vision-ga2-2022/test/test_set.csv', index_col=\"Id\")\n",
    "test_df[\"img\"] = [np.load('/kaggle/input/kul-h02a5a-computer-vision-ga2-2022/test/img/test_{}.npy'.format(idx)) for idx, _ in test_df.iterrows()]\n",
    "test_df[\"seg\"] = [-1 * np.ones(img.shape[:2], dtype=np.int8) for img in test_df[\"img\"]]\n",
    "print(\"The test set contains {} examples.\".format(len(test_df)))\n",
    "\n",
    "# The test dataframe is similar to the training dataframe, but here the values are -1 --> your task is to fill in these as good as possible in Sect. 2 and Sect. 3; in Sect. 6 this dataframe is automatically transformed in the submission CSV!\n",
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b36a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some examples\n",
    "fig, axs = plt.subplots(2, 20, figsize=(10 * 20, 10 * 2))\n",
    "for i, label in enumerate(labels):\n",
    "    df = train_df.loc[train_df[label] == 1]\n",
    "    axs[0, i].imshow(df.iloc[0][\"img\"], vmin=0, vmax=255)\n",
    "    axs[0, i].set_title(\"\\n\".join(label for label in labels if df.iloc[0][label] == 1), fontsize=40)\n",
    "    axs[0, i].axis(\"off\")\n",
    "    axs[1, i].imshow(df.iloc[0][\"seg\"], vmin=0, vmax=20)  # with the absolute color scale it will be clear that the arrays in the \"seg\" column are label maps (labels in [0, 20])\n",
    "    axs[1, i].axis(\"off\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baeb4e3",
   "metadata": {
    "papermill": {
     "duration": 0.197841,
     "end_time": "2022-04-12T14:49:02.437252",
     "exception": false,
     "start_time": "2022-04-12T14:49:02.239411",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.3 Your Kaggle submission\n",
    "Your filled test dataframe (during Sect. 2 and Sect. 3) must be converted to a submission.csv with two rows per example (one for classification and one for segmentation) and with only a single prediction column (the multi-class/label predictions running length encoded). You don't need to edit this section. Just make sure to call this function at the right position in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5956182",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-12T14:49:02.846246Z",
     "iopub.status.busy": "2022-04-12T14:49:02.844821Z",
     "iopub.status.idle": "2022-04-12T14:49:02.847765Z",
     "shell.execute_reply": "2022-04-12T14:49:02.848354Z",
     "shell.execute_reply.started": "2022-04-12T14:43:54.720774Z"
    },
    "papermill": {
     "duration": 0.213344,
     "end_time": "2022-04-12T14:49:02.848597",
     "exception": false,
     "start_time": "2022-04-12T14:49:02.635253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _rle_encode(img):\n",
    "    \"\"\"\n",
    "    Kaggle requires RLE encoded predictions for computation of the Dice score (https://www.kaggle.com/lifa08/run-length-encode-and-decode)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img: np.ndarray - binary img array\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    rle: String - running length encoded version of img\n",
    "    \"\"\"\n",
    "    pixels = img.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    rle = ' '.join(str(x) for x in runs)\n",
    "    return rle\n",
    "\n",
    "def generate_submission(df):\n",
    "    \"\"\"\n",
    "    Make sure to call this function once after you completed Sect. 2 and Sect. 3! It transforms and writes your test dataframe into a submission.csv file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame - filled dataframe that needs to be converted\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    submission_df: pd.DataFrame - df in submission format.\n",
    "    \"\"\"\n",
    "    df_dict = {\"Id\": [], \"Predicted\": []}\n",
    "    for idx, _ in df.iterrows():\n",
    "        df_dict[\"Id\"].append(f\"{idx}_classification\")\n",
    "        df_dict[\"Predicted\"].append(_rle_encode(np.array(df.loc[idx, labels])))\n",
    "        df_dict[\"Id\"].append(f\"{idx}_segmentation\")\n",
    "        df_dict[\"Predicted\"].append(_rle_encode(np.array([df.loc[idx, \"seg\"] == j + 1 for j in range(len(labels))])))\n",
    "    \n",
    "    submission_df = pd.DataFrame(data=df_dict, dtype=str).set_index(\"Id\")\n",
    "    submission_df.to_csv(\"submission.csv\")\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2adb1e",
   "metadata": {
    "papermill": {
     "duration": 0.196641,
     "end_time": "2022-04-12T14:49:03.240824",
     "exception": false,
     "start_time": "2022-04-12T14:49:03.044183",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Image classification\n",
    "The goal here is simple: implement a classification CNN and train it to recognise all 20 classes (and/or background) using the training set and compete on the test set (by filling in the classification columns in the test dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1a05c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from keras import backend as K\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from keras.preprocessing import image  \n",
    "from keras.layers import Activation, MaxPooling2D, BatchNormalization, Conv2D, Dense, GlobalAveragePooling2D, Dropout, Flatten\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335f5cc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-12T14:49:03.634747Z",
     "iopub.status.busy": "2022-04-12T14:49:03.633734Z",
     "iopub.status.idle": "2022-04-12T14:49:07.138819Z",
     "shell.execute_reply": "2022-04-12T14:49:07.138115Z",
     "shell.execute_reply.started": "2022-04-12T14:44:00.768260Z"
    },
    "papermill": {
     "duration": 3.702597,
     "end_time": "2022-04-12T14:49:07.139020",
     "exception": false,
     "start_time": "2022-04-12T14:49:03.436423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RandomClassificationModel:\n",
    "    \"\"\"\n",
    "    Random classification model: \n",
    "        - generates random labels for the inputs based on the class distribution observed during training\n",
    "        - assumes an input can have multiple labels\n",
    "    \"\"\"\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Adjusts the class ratio variable to the one observed in y. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: list of arrays - n x (height x width x 3)\n",
    "        y: list of arrays - n x (nb_classes)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        self.distribution = np.mean(y, axis=0)\n",
    "        print(\"Setting class distribution to:\\n{}\".format(\"\\n\".join(f\"{label}: {p}\" for label, p in zip(labels, self.distribution))))\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts for each input a label.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: list of arrays - n x (height x width x 3)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred: list of arrays - n x (nb_classes)\n",
    "        \"\"\"\n",
    "        np.random.seed(0)\n",
    "        return [np.array([int(np.random.rand() < p) for p in self.distribution]) for _ in X]\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.predict(X)\n",
    "    \n",
    "model = RandomClassificationModel()\n",
    "model.fit(train_df[\"img\"], train_df[labels])\n",
    "test_df.loc[:, labels] = model.predict(test_df[\"img\"])\n",
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9bd1c6",
   "metadata": {
    "papermill": {
     "duration": 0.19763,
     "end_time": "2022-04-12T14:49:07.536010",
     "exception": false,
     "start_time": "2022-04-12T14:49:07.338380",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Semantic segmentation\n",
    "The goal here is to implement a segmentation CNN that labels every pixel in the image as belonging to one of the 20 classes (and/or background). Use the training set to train your CNN and compete on the test set (by filling in the segmentation column in the test dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e5107d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-12T14:49:07.959556Z",
     "iopub.status.busy": "2022-04-12T14:49:07.958718Z",
     "iopub.status.idle": "2022-04-12T14:49:19.557423Z",
     "shell.execute_reply": "2022-04-12T14:49:19.556446Z",
     "shell.execute_reply.started": "2022-04-12T14:44:09.855923Z"
    },
    "papermill": {
     "duration": 11.823715,
     "end_time": "2022-04-12T14:49:19.557577",
     "exception": false,
     "start_time": "2022-04-12T14:49:07.733862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RandomSegmentationModel:\n",
    "    \"\"\"\n",
    "    Random segmentation model: \n",
    "        - generates random label maps for the inputs based on the class distributions observed during training\n",
    "        - every pixel in an input can only have one label\n",
    "    \"\"\"\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Adjusts the class ratio variable to the one observed in Y. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: list of arrays - n x (height x width x 3)\n",
    "        Y: list of arrays - n x (height x width)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        self.distribution = np.mean([[np.sum(Y_ == i) / Y_.size for i in range(len(labels) + 1)] for Y_ in Y], axis=0)\n",
    "        print(\"Setting class distribution to:\\nbackground: {}\\n{}\".format(self.distribution[0], \"\\n\".join(f\"{label}: {p}\" for label, p in zip(labels, self.distribution[1:]))))\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts for each input a label map.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: list of arrays - n x (height x width x 3)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Y_pred: list of arrays - n x (height x width)\n",
    "        \"\"\"\n",
    "        np.random.seed(0)\n",
    "        return [np.random.choice(np.arange(len(labels) + 1), size=X_.shape[:2], p=self.distribution) for X_ in X]\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.predict(X)\n",
    "    \n",
    "model = RandomSegmentationModel()\n",
    "model.fit(train_df[\"img\"], train_df[\"seg\"])\n",
    "test_df.loc[:, \"seg\"] = model.predict(test_df[\"img\"])\n",
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88b4b90",
   "metadata": {},
   "source": [
    "# Summary of Various Tests using Keras package\n",
    "\n",
    "First, as suggested in the assignment's instructions, we tried to implement a deep learning network using tensorflow and keras packages.\n",
    "\n",
    "**Pre-processing of Data:** \n",
    "\n",
    "A SegmentationDataset class was created to correctly upload all images and their corresponding segmentation masks as mini-batches, after applying a resize operation in order to have a fixed identical size for all inputs in our model. As usual, the files received for training were separated into a training and a validation subsets. Of note, for the segmentation task, in addition to the 20 classes available in the dataset, we thus have an extra one defined for the background (class = 0) giving us a total of 21 classes.\n",
    "\n",
    "**Models for Image Segmentation:**\n",
    "\n",
    "In opposite to a more simple image classification problem, image segmentation problems suffer a lot from the loss in resolution occurring in standard convolutional neural networks through the pooling and upsampling layers. Although it doesn't affect the classification task as much since more global features are enough, it does affect the segmentation quite a lot since fine details are important to classify each pixel correctly. Different architectures for CNNs have been developped in such a way to keep both local information from earlier layers and global information from deeper layers. Two of them are the SegNet and the U-Net, and those are the ones we tried first.\n",
    "\n",
    "The U-Net architecture, originally proposed by Ronneberger et al. (2015)[1] for biomedical image segmentation, is composed of an encoder, a decoder and some skip connections. The idea is that the encoder path captures the context while the skip connections path captures the exact localization. We followed the same network architecture as the one that was described in their paper. The contracting path is similar to a typical CNN architecture, i.e. composed of repeated steps of two convolutional layers followed by ReLU activation plus a max pooling operation. On the other side, each step of the expansive path is composed of: an upsampling and unconvolution operation, followed by a concatenation with the corresponding feature map from the contracting path (through the skip connections) and finally two convolutional layers followed by ReLU activation.\n",
    "\n",
    "The SegNet architecture, originally proposed by Badrinarayanan et al. (2016)[2] for road scene understanding applications is also composed of an encoder, a decoder and some additional direct connections between encoder and decoder layers. The idea behind the SegNet is that each decoder step uses the max-pooling indices coming from the corresponding encoder step. The encoder part follows the architecture of the convolutional layers of the VGG16 (excluding thus the fully connected layers). The reusing of the max pooling indices in the decoder part reduces the total number of parameters, making the training of SegNet faster than other architectures.\n",
    "\n",
    "**Training of Models:**\n",
    "\n",
    "Our first attempts of training those two models were done using the predefined Categorical Crossentropy loss function <span style=\"color:red\"> [@TJALLING: did you try other predefined loss function?] </span> as well as a customized Dice Loss function. With the two of them (testing on a quite small number of epoch), we got stucked with models that were predicting all pixels as background. \n",
    "\n",
    "As we couldn't find a predefined weighted Categorical Crossentropy in the keras library, we adapted our customized Dice Loss function in order to take the class_weights into account. Then, we also gave more importance to the mistake done on foreground classes compared to background class by adding an extra penalty on the loss. We tested different versions of this customized function, changing for example the weight of the penalty, or how the background class was taken into account. It did improve our results compared to the original Dice Loss function as the model was able to predict different classes. Visually, we could recognize shapes of different objects in the segmentation mask of the images. However, the actual classes chosen didn't really make sense. Looking at the training loss curves, it looks like the models weren't really learning correctly. \n",
    "\n",
    "Lastly, we also gave new attempts coming back to more a standard loss function that would take weights imbalance into account, and tried to implement the Generalized Dice Loss function as described in M.J. Cardoso et al. (2017)[3], with no success as we were back to background being predicted for all pixels.\n",
    "\n",
    "At that point, and after hours of training models, we decided to switch to PyTorch package because we have more experience of it through our master thesis. And we started testing other models with more classic weighted loss function, and optimization of other hyper parameters, see next section.\n",
    "\n",
    "As documentation, our codes were kept in an appendices section 6 at the end of this notebook.\n",
    "\n",
    "-----------\n",
    "REFERENCES: \n",
    "* [1] Ronneberger et al., U-Net: Convolutional Networks for Biomedical Image Segmentation, 2015.\n",
    "* [2] Badrinarayanan et al., SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation, 2016.\n",
    "* [3] Cardoso et al., Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1b4cd0",
   "metadata": {},
   "source": [
    "# Segmentation Models using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ed531e",
   "metadata": {},
   "source": [
    "# Metrics:\n",
    "\n",
    "After doing the first attempts in Keras looking at AUC/Recall/Precision metrics, we realized that Intersection over Union (IoU) was actually the primary metric to evaluate segmentation task accuracy. This metric is defined as the set of correct predicted labels divided by union set of predicted labels and ground truth labels. It thus measures how well the entire set of predicted labels covered the set of ground truth labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d58f538",
   "metadata": {
    "papermill": {
     "duration": 0.196901,
     "end_time": "2022-04-12T14:49:19.951018",
     "exception": false,
     "start_time": "2022-04-12T14:49:19.754117",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submit to competition\n",
    "You don't need to edit this section. Just use it at the right position in the notebook. See the definition of this function in Sect. 1.3 for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffdad71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-12T14:49:20.353462Z",
     "iopub.status.busy": "2022-04-12T14:49:20.352410Z",
     "iopub.status.idle": "2022-04-12T14:50:41.323307Z",
     "shell.execute_reply": "2022-04-12T14:50:41.324185Z",
     "shell.execute_reply.started": "2022-04-12T14:45:07.137454Z"
    },
    "papermill": {
     "duration": 81.176133,
     "end_time": "2022-04-12T14:50:41.324425",
     "exception": false,
     "start_time": "2022-04-12T14:49:20.148292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_submission(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d007326",
   "metadata": {
    "papermill": {
     "duration": 0.197466,
     "end_time": "2022-04-12T14:50:41.721228",
     "exception": false,
     "start_time": "2022-04-12T14:50:41.523762",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Adversarial attack\n",
    "For this part, your goal is to fool your classification and/or segmentation CNN, using an *adversarial attack*. More specifically, the goal is build a CNN to perturb test images in a way that (i) they look unperturbed to humans; but (ii) the CNN classifies/segments these images in line with the perturbations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5e6bad",
   "metadata": {
    "papermill": {
     "duration": 0.195695,
     "end_time": "2022-04-12T14:50:42.117581",
     "exception": false,
     "start_time": "2022-04-12T14:50:41.921886",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Discussion\n",
    "Finally, take some time to reflect on what you have learned during this assignment. Reflect and produce an overall discussion with links to the lectures and \"real world\" computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d607c7",
   "metadata": {},
   "source": [
    "# 6. Appendices (For documentation purpose only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d105b959",
   "metadata": {},
   "source": [
    "# 6.1 Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5efe11",
   "metadata": {},
   "source": [
    "#Code to be added by Elias and Jonathan if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f2fa78",
   "metadata": {},
   "source": [
    "# 6.2. Segmentation Models - Code tested in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7fc4f5",
   "metadata": {},
   "source": [
    "#Class SegmentationDataset and Creation of training and validation datasets to be used in our models.\n",
    "\n",
    "    import tensorflow as tf\n",
    "    import os\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from skimage.transform import resize\n",
    "\n",
    "    img_files = [f for f in os.listdir(\"train/img\") if not f.startswith(\".\")]\n",
    "    n = len(img_files)\n",
    "    train_files, validation_files = train_test_split(img_files, test_size=0.2, random_state=42)\n",
    "\n",
    "    class SegmentationDataset(tf.keras.utils.Sequence):\n",
    "        def __init__(self, img_folder, seg_folder, file_list, batch_size=32, image_size=(256, 256)):\n",
    "            self.img_folder = img_folder\n",
    "            self.seg_folder = seg_folder\n",
    "            self.batch_size = batch_size\n",
    "            self.image_size = image_size\n",
    "            self.img_files = file_list\n",
    "            self.num_samples = len(self.img_files)\n",
    "            \n",
    "        def __len__(self):\n",
    "            return int(np.ceil(self.num_samples / float(self.batch_size)))\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            batch_img_files = self.img_files[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
    "            batch_imgs = []\n",
    "            batch_segs = []\n",
    "            \n",
    "            for img_file in batch_img_files:\n",
    "                img_path = os.path.join(self.img_folder, img_file)\n",
    "                seg_path = os.path.join(self.seg_folder, img_file)\n",
    "                \n",
    "                img = np.load(img_path, allow_pickle=True)\n",
    "                seg = np.load(seg_path, allow_pickle=True)\n",
    "                seg = tf.expand_dims(seg, axis=-1)\n",
    "                \n",
    "                img = resize(img, self.image_size)\n",
    "                seg = resize(seg, self.image_size, order=0, mode='reflect', cval=0, clip=True, preserve_range=True, anti_aliasing=False, anti_aliasing_sigma=None)\n",
    "                \n",
    "                seg = tf.keras.utils.to_categorical(seg, num_classes=21) if np.max(seg) < 21 else seg\n",
    "                \n",
    "                batch_imgs.append(img)\n",
    "                batch_segs.append(seg)\n",
    "            \n",
    "            return np.array(batch_imgs), np.array(batch_segs)\n",
    "        \n",
    "\n",
    "    img_folder = \"train/img\"\n",
    "    seg_folder = \"train/seg\"\n",
    "    batch_size = 32\n",
    "    image_size = (256, 256)\n",
    "\n",
    "    train_dataset = SegmentationDataset(img_folder, seg_folder, batch_size=batch_size, image_size=image_size, file_list=train_files)\n",
    "    val_dataset = SegmentationDataset(img_folder, seg_folder, batch_size=batch_size, image_size=image_size, file_list=validation_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e200fd",
   "metadata": {},
   "source": [
    "#Visualization of one batch of training data (images and segmentation masks)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    train_images, train_masks = train_dataset[0]    #Get one batch of training data\n",
    "\n",
    "    for i in range(len(train_images)):  #Loop over the images in this batch\n",
    "        img = train_images[i]\n",
    "        mask = train_masks[i]\n",
    "        \n",
    "        mask = np.argmax(mask, axis=-1)  #Convert the mask which is a one-hot encoded matrix to a single channel matrix\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2)\n",
    "        \n",
    "        #Plot the original image\n",
    "        axes[0].imshow(img)\n",
    "        axes[0].set_title('Original Image')\n",
    "        \n",
    "        #Plot the ground truth segmentation mask\n",
    "        axes[1].imshow(mask, cmap='jet')\n",
    "        axes[1].set_title('Ground Truth Segmentation Mask')\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0685f13b",
   "metadata": {},
   "source": [
    "#Class weights\n",
    "\n",
    "    #Count the number of instances for each class\n",
    "    class_counts = train_df[labels].sum()\n",
    "\n",
    "    #Calculate class weights\n",
    "    class_weights = {i: 1.0/count for i, count in enumerate(class_counts)}\n",
    "\n",
    "    #print(\"Class weights: \", class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Loss Functions:\n",
    "\n",
    "    \n",
    "\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import backend as K\n",
    "    from keras import metrics\n",
    "\n",
    "    def dice_loss_ignore_background(y_true, y_pred):\n",
    "        smooth = 1e-7\n",
    "        # Exclude background class (class 0) from calculations\n",
    "        y_true_no_background = y_true[..., 1:]\n",
    "        y_pred_no_background = y_pred[..., 1:]\n",
    "        intersection = K.sum(K.abs(y_true_no_background * y_pred_no_background), axis=-1)\n",
    "        return 1 - (2.0 * intersection + smooth) / (K.sum(K.square(y_true_no_background), -1) + K.sum(K.square(y_pred_no_background), -1) + smooth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb5bf8e",
   "metadata": {},
   "source": [
    "#Class weights\n",
    "\n",
    "    train_images, train_masks = train_dataset[0]\n",
    "\n",
    "    class_weights = np.concatenate((np.arange(21), np.zeros(21)),axis=None).reshape((2, 21)).T\n",
    "    #class_weights = np.concatenate((np.arange(20), np.zeros(20)),axis=None).reshape((2, 20)).T  #for no bg\n",
    "\n",
    "    n = 0\n",
    "\n",
    "    #For each image in the batch\n",
    "    for i in range(len(train_images)):\n",
    "        mask = train_masks[i]\n",
    "        mask = np.argmax(mask, axis=-1)\n",
    "\n",
    "        unique, counts = np.unique(mask, return_counts=True)\n",
    "        #unique = unique[1:] #for no bg\n",
    "        #counts = counts[1:] #for no bg\n",
    "        sum = np.sum(counts)\n",
    "\n",
    "        weights = np.asarray((unique, counts/sum*100)).T\n",
    "        \n",
    "        for weight in weights:\n",
    "            class_id = int(weight[0])  # Get the class ID\n",
    "            count_percentage = weight[1]  # Get the count/percentage\n",
    "            \n",
    "            # Find the row in `empty_class` with the matching class ID\n",
    "            row_index = np.where(class_weights[:, 0] == class_id)[0][0]\n",
    "            #row_index = np.where(class_weights[:, 0] == class_id-1)[0][0]   #for no bg\n",
    "            \n",
    "            # Update the second column with the new count/percentage\n",
    "            class_weights[row_index, 1] += count_percentage\n",
    "\n",
    "        n +=1\n",
    "\n",
    "    class_weights[:,1] = class_weights[:,1]/n\n",
    "\n",
    "    final_weights = class_weights[:,1]/100\n",
    "    #final_weights_nobg = class_weights[:,1]/100 #for no bg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b267eeeb",
   "metadata": {},
   "source": [
    "#Tried Generalized Dice Loss + including a penalty for wrongly classifying foreground classes\n",
    "\n",
    "#+ tried the possibility to also weight the class in the foreground penalty using final_weights computed above\n",
    "\n",
    "    import tensorflow as tf\n",
    "    import keras.backend as K\n",
    "\n",
    "    def generalized_dice_loss(y_true, y_pred, init_weights=final_weights[1:]):\n",
    "        #Number of classes\n",
    "        num_classes = tf.shape(y_pred)[-1]\n",
    "        \n",
    "        #Flatten predictions and labels\n",
    "        y_true_flat = tf.reshape(y_true, (-1, num_classes))\n",
    "        y_pred_flat = tf.reshape(y_pred, (-1, num_classes))\n",
    "        \n",
    "        #Compute class weights based on the frequency of each class\n",
    "        class_weights = 1.0 / (tf.reduce_sum(y_true_flat, axis=0) ** 2)\n",
    "        class_weights = tf.where(tf.math.is_finite(class_weights), class_weights, 1e-10)\n",
    "        print(class_weights)\n",
    "        \n",
    "        #Compute Dice score for each class\n",
    "        numerator = 2.0 * tf.reduce_sum(y_true_flat * y_pred_flat, axis=0)\n",
    "        denominator = tf.reduce_sum(y_true_flat + y_pred_flat, axis=0)\n",
    "        class_dice_scores = (numerator + 1e-10) / (denominator + 1e-10)\n",
    "        \n",
    "        #Multiply Dice scores by class weights\n",
    "        weighted_dice_scores = class_weights * class_dice_scores\n",
    "\n",
    "        #Compute Generalized Dice Loss\n",
    "        dice_loss = 1.0 - tf.reduce_sum(weighted_dice_scores) / (tf.reduce_sum(class_weights) + 1e-10)\n",
    "        \n",
    "        #Penalize incorrect predictions for foreground classes\n",
    "        fg_penalty = 1.0 - class_dice_scores[1:]  # Exclude background class\n",
    "        foreground_loss = tf.reduce_mean(fg_penalty)\n",
    "        #foreground_loss = tf.reduce_mean(init_weights*fg_penalty)\n",
    "        \n",
    "        #Combine Generalized Dice Loss with foreground penalty\n",
    "        total_loss = dice_loss + 0.5 * foreground_loss\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "    loss_fn = generalized_dice_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bc0e04",
   "metadata": {},
   "source": [
    "#Tried to update the Generalized Dice Loss function as in Reference [3]\n",
    "\n",
    "#From litterature review, I think Generalized Dice Loss should already compensate for class weights and background...\n",
    "\n",
    "    def new_generalized_dice_loss(y_true, y_pred):\n",
    "        # Number of classes\n",
    "        num_classes = tf.shape(y_pred)[-1]\n",
    "        \n",
    "        # Flatten predictions and labels\n",
    "        y_true_flat = tf.reshape(y_true, (-1, num_classes))\n",
    "        y_pred_flat = tf.reshape(y_pred, (-1, num_classes))\n",
    "        \n",
    "        # Compute class weights based on the frequency of each class\n",
    "        class_weights = 1.0 / (tf.reduce_sum(y_true_flat, axis=0) ** 2)\n",
    "        class_weights = tf.where(tf.math.is_finite(class_weights), class_weights, 1e-10)\n",
    "        \n",
    "        # Compute Dice score for each class\n",
    "        numerator = tf.reduce_sum(y_true_flat * y_pred_flat, axis=0)\n",
    "        denominator = tf.reduce_sum(y_true_flat + y_pred_flat, axis=0)\n",
    "        \n",
    "        weighted_num = tf.reduce_sum(class_weights * numerator, axis = -1)\n",
    "        weighted_den = tf.reduce_sum(class_weights * denominator, axis = -1)\n",
    "        \n",
    "        # Compute Generalized Dice Loss\n",
    "        dice_loss = 1.0 - 2.0 * weighted_num / weighted_den\n",
    "        #dice_loss = tf.where(tf.math.is_finite(dice_loss), dice_loss, 1e-10)\n",
    "        \n",
    "        total_loss = dice_loss\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    #loss_fn = new_generalized_dice_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4193a961",
   "metadata": {},
   "source": [
    "#U-Net model - following the structure from the original paper from Ronneberger et al. (2015)\n",
    "\n",
    "    from keras.layers import Conv2D, MaxPool2D, Dropout, concatenate, Conv2DTranspose, Input\n",
    "\n",
    "    def double_conv2D(x, n_filters):   # Two layers of Conv2D with ReLU activation\n",
    "        x = Conv2D(n_filters, 3, padding = \"same\", activation = \"relu\", kernel_initializer = \"he_normal\")(x)\n",
    "        x = Conv2D(n_filters, 3, padding = \"same\", activation = \"relu\", kernel_initializer = \"he_normal\")(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def downsample(x, n_filters):\n",
    "        f = double_conv2D(x, n_filters)\n",
    "        p = MaxPool2D(2)(f)\n",
    "        p = Dropout(0.3)(p)\n",
    "\n",
    "        return f, p\n",
    "\n",
    "    def upsample(x, conv_features, n_filters):\n",
    "        x = Conv2DTranspose(n_filters, 3, 2, padding=\"same\")(x)\n",
    "        x = concatenate([x, conv_features])\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = double_conv2D(x, n_filters)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def build_unet():\n",
    "        inputs = Input(shape=(256,256,3))\n",
    "\n",
    "        #Encoder part: \n",
    "        f1, p1 = downsample(inputs, 64)\n",
    "        f2, p2 = downsample(p1, 128)\n",
    "        f3, p3 = downsample(p2, 256)\n",
    "        f4, p4 = downsample(p3, 512)\n",
    "\n",
    "        #Bottleneck part:\n",
    "        bottleneck = double_conv2D(p4, 1024)\n",
    "\n",
    "        #Decoder part\n",
    "        u6 = upsample(bottleneck, f4, 512)\n",
    "        u7 = upsample(u6, f3, 256)\n",
    "        u8 = upsample(u7, f2, 128)\n",
    "        u9 = upsample(u8, f1, 64)\n",
    "\n",
    "        outputs = Conv2D(21, 1, padding=\"same\", activation = \"softmax\")(u9)\n",
    "\n",
    "        unet_model = tf.keras.Model(inputs, outputs, name=\"U-Net\")\n",
    "\n",
    "        return unet_model\n",
    "\n",
    "    unet_model = build_unet()\n",
    "    \n",
    "    unet_model.compile(optimizer='adam', loss=loss_fn, metrics=[metrics.Precision(), metrics.Recall(), metrics.AUC()])\n",
    "\n",
    "    history = unet_model.fit(train_dataset, epochs=10, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3822943",
   "metadata": {},
   "source": [
    "#SegNet model:\n",
    "\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
    "    from tensorflow.keras import regularizers\n",
    "\n",
    "    def segnet(input_shape, n_classes):\n",
    "        inputs = Input(input_shape)\n",
    "\n",
    "        # Encoder\n",
    "        conv1 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(inputs)\n",
    "        pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "        conv2 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(pool1)\n",
    "        pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "        conv3 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(pool2)\n",
    "        pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "        # Decoder\n",
    "        up4 = UpSampling2D(size=(2, 2))(pool3)\n",
    "        conv4 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(up4)\n",
    "\n",
    "        up5 = UpSampling2D(size=(2, 2))(conv4)\n",
    "        conv5 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(up5)\n",
    "\n",
    "        up6 = UpSampling2D(size=(2, 2))(conv5)\n",
    "        conv6 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(up6)\n",
    "\n",
    "        conv7 = Conv2D(n_classes, (1, 1), activation='softmax')(conv6)\n",
    "\n",
    "        return Model(inputs=[inputs], outputs=[conv7])\n",
    "\n",
    "\n",
    "    #Instantiate the model\n",
    "    Segnet = segnet((256, 256, 3), 21)  # Assuming 20 classes and image dimensions are 256x256, in RGB \n",
    "    Segnet.compile(optimizer='adam', loss=dice_loss_ignore_background, metrics=[metrics.Precision(), metrics.Recall(), metrics.AUC()])\n",
    "\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "    #Define the early stopping criteria\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3)  # stop training when 'val_loss' has stopped improving for 3 epochs\n",
    "\n",
    "    #Define the model checkpoint criteria\n",
    "    model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)  # save only the best model to 'best_model.h5'\n",
    "\n",
    "    #Add the model checkpoint callback to the fit function along with early stopping\n",
    "    segnet_history = Segnet.fit(train_dataset, epochs=10, callbacks=[early_stop, model_checkpoint])\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    #Plot training & validation accuracy values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(segnet_history.history['auc'])\n",
    "    plt.title('Model AUC')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train'], loc='upper left')\n",
    "\n",
    "    #Plot training & validation loss values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(segnet_history.history['loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train'], loc='upper left')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c4298a",
   "metadata": {},
   "source": [
    "#Create TestDataset class:\n",
    "\n",
    "    class TestDataset(tf.keras.utils.Sequence):\n",
    "        def __init__(self, img_folder, file_list, batch_size=32, image_size=(256, 256)):\n",
    "            self.img_folder = img_folder\n",
    "            self.batch_size = batch_size\n",
    "            self.image_size = image_size\n",
    "            self.img_files = file_list\n",
    "            self.num_samples = len(self.img_files)\n",
    "            \n",
    "        def __len__(self):\n",
    "            return int(np.ceil(self.num_samples / float(self.batch_size)))\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            batch_img_files = self.img_files[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
    "            batch_imgs = []\n",
    "            \n",
    "            for img_file in batch_img_files:\n",
    "                img_path = os.path.join(self.img_folder, img_file)\n",
    "                \n",
    "                img = np.load(img_path, allow_pickle=True)\n",
    "                img = resize(img, self.image_size)\n",
    "                \n",
    "                batch_imgs.append(img)\n",
    "            \n",
    "            return np.array(batch_imgs)\n",
    "\n",
    "    #Assuming you have a list of test files and the test images are in the 'test/img' folder\n",
    "    test_img_folder = \"test/img\"\n",
    "    test_files = [f for f in os.listdir(test_img_folder) if not f.startswith(\".\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12eb2b0",
   "metadata": {},
   "source": [
    "#Predict on test with unet or segnet:\n",
    "\n",
    "    test_dataset = TestDataset(test_img_folder, batch_size=batch_size, image_size=image_size, file_list=test_files)\n",
    "    predictions = unet_model.predict(test_dataset)\n",
    "    \n",
    "    test_dataset = TestDataset(test_img_folder, batch_size=batch_size, image_size=image_size, file_list=test_files)\n",
    "    predictions = Segnet.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c2157e",
   "metadata": {},
   "source": [
    "#Plot predicted masks on test data:\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    #Get a batch of data from the test dataset\n",
    "    test_images = test_dataset[0]\n",
    "\n",
    "    #Get the corresponding predictions\n",
    "    test_predictions = predictions[:len(test_images)]\n",
    "\n",
    "    #For each image in the batch\n",
    "    for i in range(len(test_images)):\n",
    "        # Get the image and its corresponding prediction\n",
    "        img = test_images[i]\n",
    "        pred = test_predictions[i]\n",
    "        \n",
    "        # The prediction is a one-hot encoded matrix, we need to convert it to a single channel matrix\n",
    "        pred = np.argmax(pred, axis=-1)\n",
    "        \n",
    "        # Create a figure with two subplots\n",
    "        fig, axes = plt.subplots(1, 2)\n",
    "        \n",
    "        # Plot the original image in the first subplot\n",
    "        axes[0].imshow(img)\n",
    "        axes[0].set_title('Original Image')\n",
    "        \n",
    "        # Plot the predicted segmentation mask in the second subplot\n",
    "        axes[1].imshow(pred, cmap='jet')\n",
    "        axes[1].set_title('Predicted Segmentation Mask')\n",
    "        \n",
    "        # Show the figure\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 151.961305,
   "end_time": "2022-04-12T14:50:45.298914",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-04-12T14:48:13.337609",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
